"""
neighborhoods.py - Neighborhood composition and niche identification

Analyzes the cellular microenvironment around each cell using
polygon adjacency from graph.py:
- What cell types surround each cell
- Are certain cell type contacts enriched (permutation test)
- Identify spatial niches by clustering neighborhood profiles
- Detect cells at boundaries between tissue regions
"""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from spatioloji_s.data.core import spatioloji

import numpy as np
import pandas as pd

from .graph import PolygonSpatialGraph


def neighborhood_composition(
    sp: spatioloji, graph: PolygonSpatialGraph, group_col: str, mode: str = "fraction", store: bool = True
) -> pd.DataFrame:
    """
    Compute cell type composition of each cell's polygon neighborhood.

    For each cell, looks at its neighbors in the polygon graph and
    counts/fractions of each cell type among those neighbors.

    Parameters
    ----------
    sp : spatioloji
        spatioloji object
    graph : PolygonSpatialGraph
        Pre-built polygon graph
    group_col : str
        Column in cell_meta defining cell types
    mode : str
        'count': raw neighbor counts per type
        'fraction': normalized to sum to 1.0
    store : bool
        If True, store composition columns in sp.cell_meta
        with prefix 'nhood_{group_col}_'

    Returns
    -------
    pd.DataFrame
        Rows = cells, columns = cell types, values = count or fraction

    Examples
    --------
    >>> graph = build_contact_graph(sp)
    >>> comp = neighborhood_composition(sp, graph, group_col='cell_type')
    >>> # What fraction of cell_001's neighbors are T cells?
    >>> comp.loc['cell_001', 'T_cell']
    """
    print(f"\n[Neighborhoods] Computing composition by '{group_col}'...")

    if group_col not in sp.cell_meta.columns:
        raise ValueError(f"'{group_col}' not found in cell_meta")

    if mode not in ("count", "fraction"):
        raise ValueError(f"mode must be 'count' or 'fraction', got '{mode}'")

    # Get cell type labels
    labels = sp.cell_meta[group_col]
    unique_types = sorted(labels.dropna().unique())
    type_to_idx = {t: i for i, t in enumerate(unique_types)}
    n_types = len(unique_types)

    # Build composition matrix (n_cells × n_types)
    cell_index = graph.cell_index
    n_cells = len(cell_index)
    comp_matrix = np.zeros((n_cells, n_types), dtype=np.float64)

    adj = graph.adjacency

    for i in range(n_cells):
        neighbor_indices = adj[i].nonzero()[1]

        if len(neighbor_indices) == 0:
            continue

        for ni in neighbor_indices:
            neighbor_id = cell_index[ni]
            if neighbor_id in labels.index:
                ntype = labels.loc[neighbor_id]
                if pd.notna(ntype) and ntype in type_to_idx:
                    comp_matrix[i, type_to_idx[ntype]] += 1

    # Normalize to fractions
    if mode == "fraction":
        row_sums = comp_matrix.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1.0  # avoid division by zero
        comp_matrix = comp_matrix / row_sums

    # Build DataFrame
    comp_df = pd.DataFrame(comp_matrix, index=cell_index, columns=unique_types)

    # Reindex to full cell_index
    comp_df = comp_df.reindex(sp.cell_index, fill_value=0.0)

    if store:
        prefix = f"nhood_{group_col}_"
        for col in comp_df.columns:
            sp.cell_meta[f"{prefix}{col}"] = comp_df[col].values
        print(f"  ✓ Stored {n_types} columns with prefix '{prefix}'")

    print(f"  ✓ Composition: {n_cells} cells, {n_types} types, mode='{mode}'")

    return comp_df


def neighborhood_enrichment(
    sp: spatioloji, graph: PolygonSpatialGraph, group_col: str, n_permutations: int = 1000, seed: int = 42
) -> dict[str, pd.DataFrame]:
    """
    Test enrichment/depletion of cell type contacts via permutation.

    For each pair of cell types (A, B), counts the number of A-B
    contacts in the polygon graph, then compares to a null distribution
    generated by randomly shuffling cell type labels.

    Parameters
    ----------
    sp : spatioloji
        spatioloji object
    graph : PolygonSpatialGraph
        Pre-built polygon graph
    group_col : str
        Column in cell_meta defining cell types
    n_permutations : int
        Number of label permutations for null distribution
    seed : int
        Random seed for reproducibility

    Returns
    -------
    dict with keys:
        'z_scores' : pd.DataFrame
            Z-score matrix (positive = enriched, negative = depleted)
        'p_values' : pd.DataFrame
            Two-sided p-value matrix
        'observed' : pd.DataFrame
            Observed contact count matrix
        'expected' : pd.DataFrame
            Mean expected contact count matrix (from permutations)

    Examples
    --------
    >>> graph = build_contact_graph(sp)
    >>> results = neighborhood_enrichment(sp, graph, group_col='cell_type')
    >>> print(results['z_scores'])
    >>> # Positive z-score = cell types contact more than expected
    >>> # Negative z-score = cell types avoid each other
    """
    print(f"\n[Neighborhoods] Enrichment analysis (n_perm={n_permutations})...")

    if group_col not in sp.cell_meta.columns:
        raise ValueError(f"'{group_col}' not found in cell_meta")

    # Get labels aligned to graph cell_index
    labels = sp.cell_meta[group_col].reindex(graph.cell_index)
    valid_mask = labels.notna()

    if valid_mask.sum() < 2:
        raise ValueError("Need at least 2 cells with labels")

    unique_types = sorted(labels.dropna().unique())
    n_types = len(unique_types)
    type_to_idx = {t: i for i, t in enumerate(unique_types)}

    # Convert labels to integer array for fast permutation
    label_array = np.full(len(labels), -1, dtype=np.int32)
    for i, cell_id in enumerate(graph.cell_index):
        lbl = labels.loc[cell_id]
        if pd.notna(lbl) and lbl in type_to_idx:
            label_array[i] = type_to_idx[lbl]

    # Get edges as integer pairs (upper triangle)
    rows, cols = graph.adjacency.nonzero()
    mask = rows < cols
    edge_rows = rows[mask]
    edge_cols = cols[mask]

    # Function to count contacts for a given label assignment
    def _count_contacts(lab_arr):
        counts = np.zeros((n_types, n_types), dtype=np.float64)
        for r, c in zip(edge_rows, edge_cols, strict=True):
            t_r = lab_arr[r]
            t_c = lab_arr[c]
            if t_r >= 0 and t_c >= 0:
                counts[t_r, t_c] += 1
                counts[t_c, t_r] += 1  # symmetric
        return counts

    # Observed contacts
    print("  → Counting observed contacts...")
    observed = _count_contacts(label_array)

    # Permutation null distribution
    print(f"  → Running {n_permutations} permutations...")
    rng = np.random.default_rng(seed)
    perm_counts = np.zeros((n_permutations, n_types, n_types), dtype=np.float64)

    for p in range(n_permutations):
        shuffled = label_array.copy()
        # Only shuffle cells that have valid labels
        valid_indices = np.where(shuffled >= 0)[0]
        shuffled[valid_indices] = rng.permutation(shuffled[valid_indices])
        perm_counts[p] = _count_contacts(shuffled)

    # Compute z-scores and p-values
    perm_mean = perm_counts.mean(axis=0)
    perm_std = perm_counts.std(axis=0)

    # Z-score: (observed - expected) / std
    z_scores = np.zeros((n_types, n_types), dtype=np.float64)
    nonzero_std = perm_std > 0
    z_scores[nonzero_std] = (observed[nonzero_std] - perm_mean[nonzero_std]) / perm_std[nonzero_std]

    # Two-sided p-value: fraction of permutations as extreme as observed
    p_values = np.ones((n_types, n_types), dtype=np.float64)
    for i in range(n_types):
        for j in range(n_types):
            if perm_std[i, j] > 0:
                n_extreme = np.sum(
                    np.abs(perm_counts[:, i, j] - perm_mean[i, j]) >= np.abs(observed[i, j] - perm_mean[i, j])
                )
                p_values[i, j] = (n_extreme + 1) / (n_permutations + 1)

    # Build DataFrames
    observed_df = pd.DataFrame(observed, index=unique_types, columns=unique_types)
    expected_df = pd.DataFrame(perm_mean, index=unique_types, columns=unique_types)
    z_score_df = pd.DataFrame(z_scores, index=unique_types, columns=unique_types)
    p_value_df = pd.DataFrame(p_values, index=unique_types, columns=unique_types)

    # Report
    print("  ✓ Enrichment analysis complete")
    print("\n  Significant pairs (p < 0.05):")
    for i in range(n_types):
        for j in range(i, n_types):
            if p_values[i, j] < 0.05:
                direction = "enriched" if z_scores[i, j] > 0 else "depleted"
                print(
                    f"    {unique_types[i]} – {unique_types[j]}: "
                    f"z={z_scores[i, j]:+.2f}, p={p_values[i, j]:.3f} ({direction})"
                )

    return {"z_scores": z_score_df, "p_values": p_value_df, "observed": observed_df, "expected": expected_df}


def niche_identification(
    sp: spatioloji,
    graph: PolygonSpatialGraph,
    group_col: str,
    n_niches: int = 5,
    method: str = "kmeans",
    store: bool = True,
) -> pd.Series:
    """
    Identify spatial niches by clustering neighborhood composition profiles.

    Each cell gets a neighborhood composition vector (from
    neighborhood_composition), then cells with similar local
    environments are grouped into niches.

    Parameters
    ----------
    sp : spatioloji
        spatioloji object
    graph : PolygonSpatialGraph
        Pre-built polygon graph
    group_col : str
        Column in cell_meta defining cell types
    n_niches : int
        Number of niches to identify
    method : str
        Clustering method: 'kmeans' or 'leiden'
    store : bool
        If True, store result in sp.cell_meta['niche']

    Returns
    -------
    pd.Series
        Niche label per cell

    Examples
    --------
    >>> graph = build_contact_graph(sp)
    >>> niches = niche_identification(sp, graph, group_col='cell_type', n_niches=5)
    >>> sp.cell_meta['niche'].value_counts()
    """
    print(f"\n[Neighborhoods] Identifying {n_niches} niches (method='{method}')...")

    # Get neighborhood composition (fraction mode)
    comp_df = neighborhood_composition(sp, graph, group_col, mode="fraction", store=False)

    # Only cluster cells that have neighbors
    has_neighbors = comp_df.sum(axis=1) > 0
    valid_comp = comp_df[has_neighbors]

    if len(valid_comp) < n_niches:
        raise ValueError(
            f"Only {len(valid_comp)} cells with neighbors, " f"need at least {n_niches} for {n_niches} niches."
        )

    labels = pd.Series(np.nan, index=sp.cell_index, dtype=object)

    if method == "kmeans":
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        scaled = scaler.fit_transform(valid_comp)

        km = KMeans(n_clusters=n_niches, random_state=42, n_init=10)
        cluster_labels = km.fit_predict(scaled)

        # Name niches as niche_0, niche_1, ...
        named_labels = [f"niche_{c}" for c in cluster_labels]
        labels[has_neighbors] = named_labels

        print(f"  → KMeans inertia: {km.inertia_:.1f}")

    elif method == "leiden":
        try:
            import anndata
            import scanpy as sc
        except ImportError as err:
            raise ImportError("Leiden clustering requires scanpy. " "Install with: pip install scanpy") from err

        # Build AnnData from composition, run neighbors + leiden
        adata = anndata.AnnData(X=valid_comp.values)
        adata.obs_names = valid_comp.index.astype(str)

        sc.pp.neighbors(adata, n_neighbors=15, use_rep="X")
        sc.tl.leiden(adata, resolution=n_niches / 5.0)

        cluster_labels = [f"niche_{c}" for c in adata.obs["leiden"]]
        labels[has_neighbors] = cluster_labels

    else:
        raise ValueError(f"Unknown method: '{method}'. Use 'kmeans' or 'leiden'.")

    if store:
        sp.cell_meta["niche"] = labels.values
        print("  ✓ Stored in cell_meta['niche']")

    # Report
    counts = labels.value_counts(dropna=False)
    print("  ✓ Niche identification results:")
    for niche, n in counts.items():
        label = niche if pd.notna(niche) else "unassigned"
        print(f"    {label}: {n:,} cells")

    return labels


def boundary_cells(
    sp: spatioloji, graph: PolygonSpatialGraph, group_col: str, min_different_fraction: float = 0.3, store: bool = True
) -> pd.Series:
    """
    Identify cells at boundaries between different cell type regions.

    A boundary cell has a significant fraction of its polygon-neighbors
    belonging to a different cell type than itself.

    Parameters
    ----------
    sp : spatioloji
        spatioloji object
    graph : PolygonSpatialGraph
        Pre-built polygon graph
    group_col : str
        Column in cell_meta defining cell types
    min_different_fraction : float
        Minimum fraction of neighbors that must be a different type
        for a cell to be classified as boundary (default 0.3)
    store : bool
        If True, store results in sp.cell_meta:
        'is_boundary': bool, 'boundary_score': float

    Returns
    -------
    pd.Series
        Boolean mask: True for boundary cells

    Examples
    --------
    >>> graph = build_contact_graph(sp)
    >>> boundaries = boundary_cells(sp, graph, group_col='cell_type')
    >>> n_boundary = boundaries.sum()
    >>> print(f"{n_boundary} boundary cells found")
    >>>
    >>> # Subset to boundary cells only
    >>> boundary_ids = sp.cell_index[boundaries].tolist()
    >>> sp_boundary = sp.subset_by_cells(boundary_ids)
    """
    print(f"\n[Neighborhoods] Detecting boundary cells " f"(threshold={min_different_fraction})...")

    if group_col not in sp.cell_meta.columns:
        raise ValueError(f"'{group_col}' not found in cell_meta")

    labels = sp.cell_meta[group_col]
    cell_index = graph.cell_index
    adj = graph.adjacency

    # Compute fraction of neighbors that are different type
    boundary_score = pd.Series(0.0, index=sp.cell_index, dtype=np.float64)

    for i in range(len(cell_index)):
        cell_id = cell_index[i]
        cell_type = labels.get(cell_id)

        if pd.isna(cell_type):
            continue

        neighbor_indices = adj[i].nonzero()[1]
        if len(neighbor_indices) == 0:
            continue

        n_different = 0
        n_valid = 0

        for ni in neighbor_indices:
            neighbor_id = cell_index[ni]
            neighbor_type = labels.get(neighbor_id)
            if pd.notna(neighbor_type):
                n_valid += 1
                if neighbor_type != cell_type:
                    n_different += 1

        if n_valid > 0:
            boundary_score.loc[cell_id] = n_different / n_valid

    is_boundary = boundary_score >= min_different_fraction

    if store:
        sp.cell_meta["boundary_score"] = boundary_score.values
        sp.cell_meta["is_boundary"] = is_boundary.values
        print("  ✓ Stored 'boundary_score' and 'is_boundary' in cell_meta")

    n_boundary = is_boundary.sum()

    print(f"  ✓ {n_boundary:,} boundary cells detected")
    print(f"    Score distribution: mean={boundary_score.mean():.3f}, " f"median={boundary_score.median():.3f}")

    # Report boundary cells per type
    if n_boundary > 0:
        boundary_types = labels[is_boundary].value_counts()
        print("\n  Boundary cells by type:")
        for t, n in boundary_types.items():
            total_of_type = (labels == t).sum()
            pct = 100 * n / total_of_type if total_of_type > 0 else 0
            print(f"    {t}: {n:,} ({pct:.1f}% of type)")

    return is_boundary


def harmonize_niches(comp_dict: dict, n_niches: int = 5, method: str = "kmeans", seed: int = 42) -> pd.Series:
    """
    Assign globally consistent niche labels across FOVs.

    Since niche_identification() runs per-FOV, niche labels are locally
    arbitrary (niche_0 in FOV1 ≠ niche_0 in FOV2). This function solves
    that by concatenating all per-FOV neighborhood composition profiles
    and clustering them globally — so labels reflect actual tissue
    environments, not FOV-specific numbering.

    Workflow
    --------
    1. Run neighborhood_composition() per FOV (store=False)
    2. Pass all results into this function
    3. Use returned global labels downstream

    Parameters
    ----------
    comp_dict : dict
        {fov_id: composition_DataFrame} where each DataFrame is the
        output of neighborhood_composition() for that FOV.
        Rows = cells, columns = cell types, values = fractions.
    n_niches : int
        Number of global niches
    method : str
        'kmeans' or 'leiden'
    seed : int
        Random seed

    Returns
    -------
    pd.Series
        Global niche label per cell (indexed by cell_id, covers all FOVs).
        Cells with no neighbors get NaN.

    Examples
    --------
    >>> comp_dict = {}
    >>> for fov_id, sp_fov in iter_fovs(sp):
    ...     graph = spoly.build_contact_graph(sp_fov)
    ...     comp = spoly.neighborhood_composition(
    ...         sp_fov, graph, group_col='leiden', store=False
    ...     )
    ...     comp_dict[fov_id] = comp
    >>>
    >>> global_niches = harmonize_niches(comp_dict, n_niches=5, method='kmeans')
    >>> # Map back to sp.cell_meta
    >>> sp.cell_meta['global_niche'] = global_niches.reindex(sp.cell_index)
    """
    print(f"\n[Neighborhoods] Harmonizing niches across {len(comp_dict)} FOVs...")

    # ── Step 1: Concatenate all FOV compositions ──────────────────────────────
    # Align columns (cell types) across FOVs — fill missing types with 0
    all_comp = pd.concat(comp_dict.values(), axis=0).fillna(0.0)

    # Only cluster cells that actually have neighbors (row sum > 0)
    has_neighbors = all_comp.sum(axis=1) > 0
    valid_comp = all_comp[has_neighbors]

    print(f"  → {len(all_comp)} total cells, {has_neighbors.sum()} with neighbors")
    print(f"  → {all_comp.shape[1]} cell types in composition space")

    if len(valid_comp) < n_niches:
        raise ValueError(f"Only {len(valid_comp)} cells with neighbors across all FOVs, " f"need at least {n_niches}.")

    # ── Step 2: Global clustering ─────────────────────────────────────────────
    labels = pd.Series(np.nan, index=all_comp.index, dtype=object)

    if method == "kmeans":
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        scaled = scaler.fit_transform(valid_comp)

        km = KMeans(n_clusters=n_niches, random_state=seed, n_init=10)
        cluster_labels = km.fit_predict(scaled)
        labels[has_neighbors] = [f"niche_{c}" for c in cluster_labels]

        print(f"  → KMeans inertia: {km.inertia_:.1f}")

    elif method == "leiden":
        try:
            import anndata
            import scanpy as sc
        except ImportError as err:
            raise ImportError("Leiden requires scanpy: pip install scanpy") from err

        adata = anndata.AnnData(X=valid_comp.values)
        adata.obs_names = valid_comp.index.astype(str)
        sc.pp.neighbors(adata, n_neighbors=15, use_rep="X")
        sc.tl.leiden(adata, resolution=n_niches / 5.0)
        labels[has_neighbors] = [f"niche_{c}" for c in adata.obs["leiden"]]

    else:
        raise ValueError(f"Unknown method: '{method}'. Use 'kmeans' or 'leiden'.")

    # ── Step 3: Report niche composition summary ──────────────────────────────
    # Show what cell types characterize each global niche
    print(f"\n  ✓ Global niche summary ({n_niches} niches):")

    valid_labels = labels[has_neighbors]
    for niche in sorted(valid_labels.dropna().unique()):
        niche_cells = valid_labels[valid_labels == niche].index
        centroid = valid_comp.loc[niche_cells].mean()
        top_types = centroid.nlargest(3)
        top_str = ", ".join(f"{t}={v:.2f}" for t, v in top_types.items() if v > 0)
        # FOV distribution
        fov_counts = {}
        for fov_id, comp in comp_dict.items():
            n = (niche_cells.isin(comp.index)).sum()
            if n > 0:
                fov_counts[fov_id] = n
        print(f"    {niche}: {len(niche_cells)} cells | top types: {top_str}")
        print(f"      FOVs: { {k: v for k, v in fov_counts.items()} }")

    return labels
